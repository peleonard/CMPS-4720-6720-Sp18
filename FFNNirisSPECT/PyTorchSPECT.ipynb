{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 23)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# Import the training and testing data\n",
    "spNames = ['F' + str(i) for i in range(23)]\n",
    "spNames[0] = 'diagnosis'\n",
    "\n",
    "SP_train = pd.read_csv(\"SPECTtrain.txt\", names=spNames)\n",
    "print(SP_train.shape)\n",
    "\n",
    "SP_test = pd.read_csv(\"SPECTtest.txt\", names=spNames)\n",
    "SP_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# convert the object type from DataFrame to TorchTensor\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H1, H2, D_out = 80, SP_train.shape[1]-1, 11, 11, 1\n",
    "\n",
    "# Convert data from Pandas DataFrame to Numpy Array, \n",
    "# and then to Torch Tensor, and Then to Torch Varaible\n",
    "xTorchTensor = torch.from_numpy(np.array(SP_train.iloc[:, 1:23]))\n",
    "x = Variable(xTorchTensor.type(dtype), requires_grad=False)\n",
    "yTorchTensor = torch.from_numpy(np.array(SP_train.iloc[:,0]))\n",
    "y = Variable(yTorchTensor.type(dtype), requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error of the 1 hidden layer neural network, with learning \n",
      "      rate 0.000500 and epoch 5000, is 0.1607\n",
      "Testing error of the 1 hidden layer neural network, with learning \n",
      "      rate 0.000500 and epoch 5000, is 0.1698\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Variables for its weight and bias.\n",
    "\n",
    "#One Hidden layer fully connected\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H1, D_out)\n",
    ")\n",
    "\n",
    "# Two Hidden layers fully connected neural networks\n",
    "#model = torch.nn.Sequential(\n",
    "#    torch.nn.Linear(D_in, H1),\n",
    "#    torch.nn.Linear(H1, H1),\n",
    "#    torch.nn.Linear(H2, D_out)\n",
    "#)\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "epoch = 5000\n",
    "for t in range(epoch):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Variable of input data to the Module and it produces\n",
    "    # a Variable of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Variables containing the predicted and true\n",
    "    # values of y, and the loss function returns a Variable containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "#    print(t, loss.data[0])\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Variables with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Variable, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data\n",
    "\n",
    "print(\"\"\"Training error of the 1 hidden layer neural network, with learning \n",
    "      rate %f and epoch %d, is %.4f\"\"\" %(learning_rate,epoch,loss.data))\n",
    "\n",
    "\n",
    "xTestTorchTensor = torch.from_numpy(np.array(SP_test.iloc[:, 1:23]))\n",
    "xTest = Variable(xTestTorchTensor.type(dtype), requires_grad=False)\n",
    "\n",
    "yTestTorchTensor = torch.from_numpy(np.array(SP_test.iloc[:,0]))\n",
    "yTest = Variable(yTestTorchTensor.type(dtype), requires_grad=False) \n",
    " \n",
    "yTest_pred = model(xTest)\n",
    "testError =  loss_fn(yTest_pred, yTest) \n",
    "print(\"\"\"Testing error of the 1 hidden layer neural network, with learning \n",
    "      rate %f and epoch %d, is %.4f\"\"\" %(learning_rate,epoch,testError))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
